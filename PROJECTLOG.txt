%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
DATA PREPROCESSING AND FEATURE ENGINEERING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	-all done in feature_engineering.ipynb

	Categorical Data: interest_level(target), building_id, manager_id
		-used LabelEncoder() to create numerical representation

	Original Numerical Data: bathrooms, bedrooms, latitude, longitude, price
		-leave as is for now

	Created Numerical Data: 
		num_photos (# of photos listing has)
		num_features (# of features listed in feature string)
		num_description (# of words in description)
		created_year (year listing was created)
		created_month (month listing was created)
		created_day (day listing was created)

	Text Data: features, description, display address
		-created document term matrix using CountVectorizer()
			-currently using max_features=200
		-use pickle to save each feature into its own .dat file
			-too large to convert to dataframe
			-if using to predict with a Dataframe use sparse.hstack([dtm,DF])

	Output:
		-4 files saved to /processed/
			-1 .csv file containing numerical and categorical data
			-3 .dat file containing pickled DTM for each text feature

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
MODEL SELECTION AND HYPERPARAMETER TUNING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	First, tried a few different models using train_test_split for time and varied 
	hyperparameters to get an idea of the most accurate model. To start I used
	all numerical and categorical features from the processed.csv. Text features were 
	looked at later.

	The idea was not to do an exhaustive grid search for each classifier but to see which
	ones are worth pursuing.

	Models tried with their best accuracy:
		-accuracy evaluated using multi-class logarithmic log loss as stated on Kaggle
			-sklearn.metrics.log_loss
			-calculated using predicted probabilities for each class
			-therefore can only use model capable of predicting prob for each class
				-use clf.predict_proba() method

	RandomForestClassifier() ~1.61696    549ns  
		(n_estimators=500) ~0.60398    27.5s
		(n_estimators=2000) ~0.60169    1m49s

	LogisticRegression() ~0.70157    1.35s
		(C=2) ~0.7088    1.26s
			-no real change varying C
		-no real change varying max_iter
		(solver='sag', max_iter=2000) ~0.7494    30s
		(solver='newton-cg', max_iter=200) ~0.7078    6.07s
		(solver='lbfgs') ~0.71818    939ns
		(solver='newton-cg', multi_class='ovr') ~0.7078    5.81s

	DecisionTreeClassifier() ~11.77157    293ns
		(criterion='entropy') ~11.6877    412ns
		(splitter='random') ~12.8442    96ns

	ExtraTreesClassifier() ~1.8414    347ns
		(n_estimators=1000) ~0.6922    33.4s

	GradientBoostingClassifier() ~0.61663    7.7s
		(n_estimators=1000) ~0.5785    1m12s
		(n_estimators=2000) ~0.5821    2m27s
		(max_depth=5) ~0.5908    14.6s
		(n_estimators=1000,max_depth=5) ~0.5965    2m23s

	SVM() -takes far too long to run, and didn't make up for it with accuracy

	%%%%%%%%%%%%%%% TEXT DATA %%%%%%%%%%%%%%
	Now, looking at text features:
		-set max_features=200 in CountVectorizer to keep things managable

	FEATURES DTM

	MultinomialNB(alpha=0.01, class_prior=[0.08,0.68,0.24]) ~0.82214    10ns

	LogisticRegression() ~0.74562    1.08s

	RandomForestClassifier() ~1.6546    3.56s
		(n_estimators=100) ~1.1369    35.4s

	GradientBoostingClassifier() ~0.73268    1m25s
		(n_estimators=100) ~0.73271    1m27s
			-note: can't take in CSR, need to use .toarray() method to fit and predict
				-for larger DTMs this may be too large for memory

	DESCRIPTION DTM

	MultinomialNB() ~1.04017    17ns
	(alpha=0.01, class_prior=[0.08,0.68,0.24]) ~1.0474    16ns

	LogisticRegression() ~0.74925    3.54s

	RandomForestClassifier() ~2.2972    13.5s
	(n_estimators=100) ~0.8786    2m15s

	GradientBoostingClassifier() ~0.7306    1m39s

	DISPLAY ADDRESS DTM

	MultinomialNB() ~0.7657    8ms
	(alpha=0.01, class_prior=[0.08,0.68,0.24]) ~0.77394    8ms

	LogisticRegression() ~0.7710    214ns

	RandomForestClassifier() ~1.0389   863ns
	(n_estimators=100) ~0.94308    8.45s
	(n_estimators=1000) ~0.91218    1m26s

	GradientBoostingClasifier() ~0.77003    1m24s
	(n_estimators=100) ~0.77005    1m23s


	%%%%%% ALL FEATURES WITH TEXT DATA %%%%%%%%%%%%%%
	-use sparse.hstack([]) to combine everything

	LogisticRegression() ~0.77103    207ns

	RandomForestClassifier() ~1.035    832ms
	(n_estimators=2000) ~0.9044    2m56s

	GradientBoostingClassifier() ~0.60322    7m07s




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
SUBMITTED MODELS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%